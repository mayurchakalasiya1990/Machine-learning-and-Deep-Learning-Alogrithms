{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9vPi0Yqtywu",
        "outputId": "d15813ab-56b2-4a96-eadd-b8182063abf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ],
      "source": [
        "## Imports libs\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "print(tf.__version__) \n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import itertools\n",
        "import random\n",
        "import os\n",
        "import math # Mathematical functions \n",
        "import time\n",
        "\n",
        "ts = str(time.time())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-Processing"
      ],
      "metadata": {
        "id": "i2gGxvbjAAL6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "_y5DMxshuFMY"
      },
      "outputs": [],
      "source": [
        "def load_time_series_data_and_preprocessing(filename,f_date,t_date):\n",
        "    \n",
        "    #Load data into dataframe\n",
        "    data =  pd.read_csv(filename, header=0)\n",
        "    \n",
        "    #information about dataset\n",
        "    print(data.info())\n",
        "    \n",
        "    #missing = data[col_name].isnull()\n",
        "    #data[missing]\n",
        "    \n",
        "    #Duplicate \n",
        "    \n",
        "    print(\"Missing Values:\")\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "    print(\"Duplicate Values:\")\n",
        "    duplicate = data[data.duplicated()]\n",
        "    print(duplicate)\n",
        "    \n",
        "    \n",
        "    #Convert Date from Object to Datetime type\n",
        "    data['Date'] = pd.to_datetime(data['Date'])\n",
        "    \n",
        "    #Sorting value by Date\n",
        "    data=data.sort_values(by=\"Date\", ascending=True)\n",
        "    \n",
        "    #Setting index as Date\n",
        "    data.set_index(\"Date\",inplace=True)\n",
        "    \n",
        "    #Dataset Selection by date from dataset\n",
        "    data=data[(data.index >= f_date) & (data.index <= t_date)]\n",
        "    \n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Engineering and Scaling "
      ],
      "metadata": {
        "id": "mJ9cwxda_6ky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "dftzdwr2uJ4a"
      },
      "outputs": [],
      "source": [
        "def feature_selection_scaling(data):\n",
        "    data_prices = data.drop(['SMAVG_100d'], axis=1)\n",
        "    # We add a prediction column and set dummy values to prepare the data for scaling\n",
        "    data_prices_ext = data_prices.copy()\n",
        "    data_prices_ext['Prediction'] = data_prices_ext['Close']  \n",
        "    \n",
        "    # Get the number of rows in the data\n",
        "    nrows = data_prices.shape[0]\n",
        "    \n",
        "    # Convert the data to numpy values\n",
        "    np_data_unscaled = np.array(data_prices)\n",
        "    np_data = np.reshape(np_data_unscaled, (nrows, -1))\n",
        "    print('np_data.shape:',np_data.shape)    \n",
        "    \n",
        "    # Transform the data by scaling each feature to a range between 0 and 1\n",
        "    scaler = MinMaxScaler()\n",
        "    np_data_scaled = scaler.fit_transform(np_data_unscaled)\n",
        "    \n",
        "    # Creating a separate scaler that works on a single column for scaling predictions\n",
        "    scaler_pred = MinMaxScaler()\n",
        "    df_Close = pd.DataFrame(data_prices_ext['Close'])\n",
        "    np_Close_scaled = scaler_pred.fit_transform(df_Close)\n",
        "    \n",
        "    # Print the tail of the dataframe\n",
        "    return data_prices, data_prices_ext,np_Close_scaled,np_data_scaled,scaler_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Partition Dataset Using sliding window Technique"
      ],
      "metadata": {
        "id": "AnBYBm94_zMM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "LaR24o29uNzA"
      },
      "outputs": [],
      "source": [
        "# The RNN needs data with the format of [samples, time steps, features]\n",
        "# Here, we create N samples, sequence_length time steps per sample, and 6 features\n",
        "def partition_dataset(sequence_length, data, index_Close):\n",
        "    x, y = [], []\n",
        "    data_len = data.shape[0]\n",
        "    for i in range(sequence_length, data_len):\n",
        "        x.append(data[i-sequence_length:i,:]) #contains sequence_length values 0-sequence_length * columsn\n",
        "        y.append(data[i, index_Close]) #contains the prediction values for validation,  for single-step prediction\n",
        "    \n",
        "    # Convert the x and y to numpy arrays\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform data for Model Prediction"
      ],
      "metadata": {
        "id": "EnZNe1k1_Jku"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "UeW--I3AuQrY"
      },
      "outputs": [],
      "source": [
        "def transform_multivariate_data(data,np_data_scaled,sequence_length):\n",
        "    # Set the sequence length - this is the timeframe used to make a single prediction\n",
        "    #sequence_length = 6\n",
        "    \n",
        "    # Prediction Index\n",
        "    index_Close = data.columns.get_loc(\"Close\")\n",
        "    \n",
        "    # Split the training data into train and train data sets\n",
        "    # As a first step, we get the number of rows to train the model on 80% of the data \n",
        "    train_data_len = math.ceil(np_data_scaled.shape[0] * 0.8)\n",
        "    \n",
        "    # Create the training and test data\n",
        "    train_data = np_data_scaled[0:train_data_len, :]\n",
        "    test_data = np_data_scaled[train_data_len - sequence_length:, :]\n",
        "    \n",
        "    # Generate training data and test data\n",
        "    x_train, y_train = partition_dataset(sequence_length, train_data,index_Close)\n",
        "    x_test, y_test = partition_dataset(sequence_length, test_data,index_Close)\n",
        "\n",
        "    # Print the shapes: the result is: (rows, training_sequence, features) (prediction value, )\n",
        "    print('x_train.shape:',x_train.shape,'y_train.shape:', y_train.shape)\n",
        "    print('x_test.shape:',x_test.shape,'y_test.shape:', y_test.shape)\n",
        "        \n",
        "    # Validate that the prediction value and the input match up\n",
        "    # The last close price of the second input sample should equal the first prediction value\n",
        "    print(x_train[1][sequence_length-1][index_Close])\n",
        "    print(y_train[0])\n",
        "    return  x_train, y_train, x_test, y_test, train_data_len"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evalute Model Performance "
      ],
      "metadata": {
        "id": "JraWuOxc-9Kw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "1Ru6nZ93uUOU"
      },
      "outputs": [],
      "source": [
        "def evalute_model_performance(history,output):\n",
        "    ts = str(time.time())\n",
        "    filename= output+ \"evalute_model_performance\" + ts + \".png\"\n",
        "    fig = plt.figure(figsize=(20,7))\n",
        "    fig.add_subplot(121)\n",
        "    \n",
        "    # Accuracy\n",
        "    plt.plot(history.epoch, history.history['root_mean_squared_error'], label = \"rmse\")\n",
        "    plt.plot(history.epoch, history.history['val_root_mean_squared_error'], label = \"val_rmse\")\n",
        "    \n",
        "    plt.title(\"RMSE\", fontsize=18)\n",
        "    plt.xlabel(\"Epochs\", fontsize=15)\n",
        "    plt.ylabel(\"RMSE\", fontsize=15)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.legend()\n",
        "    \n",
        "    \n",
        "    #Adding Subplot 1 (For Loss)\n",
        "    fig.add_subplot(122)\n",
        "    \n",
        "    plt.plot(history.epoch, history.history['loss'], label=\"loss\")\n",
        "    plt.plot(history.epoch, history.history['val_loss'], label=\"val_loss\")\n",
        "    \n",
        "    plt.title(\"Loss\", fontsize=18)\n",
        "    plt.xlabel(\"Epochs\", fontsize=15)\n",
        "    plt.ylabel(\"Loss\", fontsize=15)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.legend()    \n",
        "    fig.savefig(filename)\n",
        "    plt.show()  \n",
        "    \n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model with initial parameter"
      ],
      "metadata": {
        "id": "Vf2OtfoS-1fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "JmvJwB6XuZ1S"
      },
      "outputs": [],
      "source": [
        "def train_multivariate_prediction_model(x_train, y_train,filename):\n",
        "    # ------------------LSTM-----------------------\n",
        "    regressor = Sequential()\n",
        "    regressor.add(LSTM(units=16, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "    regressor.add(Dropout(0.2))\n",
        "    \n",
        "    regressor.add(LSTM(units=16, return_sequences=False))\n",
        "    regressor.add(Dropout(0.2))\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "    regressor.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    \n",
        "    print('LSTM Regression Summary :')\n",
        "    print(regressor.summary())\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "    #mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "    \n",
        "    # fit model\n",
        "    history = regressor.fit(x_train, y_train, validation_split=0.3, epochs=40, batch_size=64, callbacks=[es])\n",
        "    # plot Accuracy and Loss \n",
        "    evalute_model_performance(history,filename)\n",
        "    \n",
        "    results = regressor.evaluate(x_test, y_test)\n",
        "    print(\"test loss, test acc:\", np.round(results, 4))\n",
        "    return history, results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Model With Optimitzed Hyperparameter"
      ],
      "metadata": {
        "id": "YEB5jrzx_ONU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "558ktK_rukK0"
      },
      "outputs": [],
      "source": [
        "def train_model_with_optimized_hyperparam(n_neurons, n_batch_size, dropout,output):\n",
        "    regressor = Sequential()\n",
        "    regressor.add(LSTM(units=n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "    regressor.add(Dropout(dropout))\n",
        "        \n",
        "    regressor.add(LSTM(units=n_neurons, return_sequences=False))\n",
        "    regressor.add(Dropout(dropout))\n",
        "    regressor.add(Dense(units=1, activation='linear'))\n",
        "    regressor.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "    \n",
        "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "    \n",
        "    #file_path = output + \"/weights-{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "    file_path = \"/weights-{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "    \n",
        "    mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "    \n",
        "    history = regressor.fit(x_train, y_train, validation_split=0.3, epochs=40, batch_size=n_batch_size, callbacks=[es, mc], verbose=0)\n",
        "    #print('root_mean_squared_error:',history.history['root_mean_squared_error'])\n",
        "    #print('val_root_mean_squared_error:',history.history['val_root_mean_squared_error'])\n",
        "    \n",
        "    evalute_model_performance(history,filename)\n",
        "\n",
        "    results=regressor.evaluate(x_test, y_test)\n",
        "    print(\"test loss, test acc:\", np.round(results, 4))\n",
        "    return regressor,history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparam Tuning"
      ],
      "metadata": {
        "id": "hS8wsKHC-uVX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "yq_7uD9Luf_O"
      },
      "outputs": [],
      "source": [
        "def tune_model_hyperparam(config, x_train, y_train, x_test, y_test,output):\n",
        "    \n",
        "    n_neurons, n_batch_size, dropout = config\n",
        "    \n",
        "    possible_combinations = list(itertools.product(n_neurons, n_batch_size, dropout))\n",
        "    \n",
        "    print(possible_combinations)\n",
        "    print('\\n')\n",
        "    \n",
        "    hist = []\n",
        "    \n",
        "    for i in range(0, len(possible_combinations)):\n",
        "        \n",
        "        print(f'{i+1}th combination: \\n')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        \n",
        "        n_neurons, n_batch_size, dropout = possible_combinations[i]\n",
        "        \n",
        "        # instantiating the model in the strategy scope creates the model on the TPU\n",
        "        #with tpu_strategy.scope():\n",
        "        regressor = Sequential()\n",
        "        regressor.add(LSTM(units=n_neurons, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\n",
        "        regressor.add(Dropout(dropout))\n",
        "        \n",
        "        regressor.add(LSTM(units=n_neurons, return_sequences=False))\n",
        "        regressor.add(Dropout(dropout))\n",
        "        regressor.add(Dense(units=1, activation='linear'))\n",
        "        regressor.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "        '''''\n",
        "        From the mentioned article above --> If a validation dataset is specified to the fit() function via the validation_data or v\n",
        "        alidation_split arguments,then the loss on the validation dataset will be made available via the name “val_loss.”\n",
        "        '''''\n",
        "\n",
        "        #file_path = output + \"/weights-{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "        file_path = \"/weights-{epoch:03d}-{val_loss:.4f}.hdf5\"\n",
        "\n",
        "        mc = ModelCheckpoint(file_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
        "\n",
        "        '''''\n",
        "        cb = Callback(...)  # First, callbacks must be instantiated.\n",
        "        cb_list = [cb, ...]  # Then, one or more callbacks that you intend to use must be added to a Python list.\n",
        "        model.fit(..., callbacks=cb_list)  # Finally, the list of callbacks is provided to the callback argument when fitting the model.\n",
        "        '''''\n",
        "\n",
        "        regressor.fit(x_train, y_train, validation_split=0.3, epochs=40, batch_size=n_batch_size, callbacks=[es, mc], verbose=0)\n",
        "\n",
        "        # load the best model\n",
        "        # regressor = load_model('best_model.h5')\n",
        "\n",
        "        train_accuracy = regressor.evaluate(x_train, y_train, verbose=0)\n",
        "        test_accuracy = regressor.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "        hist.append(list((n_neurons, n_batch_size, dropout,\n",
        "                          train_accuracy, test_accuracy)))\n",
        "\n",
        "        print(f'{str(i)}-th combination = {possible_combinations[i]} \\n train accuracy: {train_accuracy} and test accuracy: {test_accuracy}')\n",
        "        \n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        print('--------------------------------------------------------------------')\n",
        "        \n",
        "\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predict Crypto coins Price"
      ],
      "metadata": {
        "id": "O7B254e3_XGA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "UNrre3uhuobM"
      },
      "outputs": [],
      "source": [
        "def predict_future_price(x_test,y_test,train_data_len,from_date,cryto_name,output,regressor):\n",
        "    ts = str(time.time())\n",
        "    filename1 = output + 'real_price_pred_price_'+ts+\".png\"\n",
        "    filename2 = output + 'y_pred_vs_y_test_'+ts+\".png\"\n",
        "    \n",
        "    y_pred_scaled = regressor.predict(x_test)\n",
        "    y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
        "\n",
        "    plt.figure(figsize=(16,8), dpi= 100, facecolor='w', edgecolor='k')\n",
        "    \n",
        "    plt.plot(y_test, color='red', label = 'Real Close Price')\n",
        "    plt.plot(y_pred_scaled, color='green', label = 'Predicted Close Price')\n",
        "    plt.legend(loc='best')\n",
        "    plt.savefig(filename1)\n",
        "\n",
        "        # Add the difference between the valid and predicted prices\n",
        "    train = pd.DataFrame(data_prices_ext['Close'][:train_data_len + 1]).rename(columns={'Close': 'y_train'})\n",
        "    valid = pd.DataFrame(data_prices_ext['Close'][train_data_len:]).rename(columns={'Close': 'y_test'})\n",
        "    valid.insert(1, \"y_pred\", y_pred, True)\n",
        "    valid.insert(1, \"residuals\", valid[\"y_pred\"] - valid[\"y_test\"], True)\n",
        "    df_union = pd.concat([train, valid])\n",
        "    # Zoom in to a closer timeframe\n",
        "    df_union_zoom = df_union[df_union.index > from_date]\n",
        "    print(df_union_zoom)    \n",
        "    df_union_zoom.to_csv(output+\"/predicted_price.csv\")\n",
        "    \n",
        "    # Create the lineplot\n",
        "    fig, ax1 = plt.subplots(figsize=(16, 8))\n",
        "    plt.title(\"y_pred vs y_test\")\n",
        "    plt.ylabel(cryto_name, fontsize=18)\n",
        "    sns.set_palette([\"#090364\", \"#1960EF\", \"#EF5919\"])\n",
        "    sns.lineplot(data=df_union_zoom[['y_pred', 'y_train', 'y_test']], linewidth=1.0, dashes=False, ax=ax1)\n",
        "    \n",
        "    # Create the bar plot with the differences\n",
        "    df_sub = [\"#2BC97A\" if x > 0 else \"#C92B2B\" for x in df_union_zoom[\"residuals\"].dropna()]\n",
        "    ax1.bar(height=df_union_zoom['residuals'].dropna(), x=df_union_zoom['residuals'].dropna().index, width=3, label='residuals', color=df_sub)\n",
        "    plt.legend()    \n",
        "    fig.savefig(filename2)\n",
        "    plt.show()\n",
        "    plt.draw()\n",
        "    \n",
        "    print('filename1:',filename1)\n",
        "    print('filename2:',filename2)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset, output path and date configuration"
      ],
      "metadata": {
        "id": "GGU_3sHIAPSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "cY6PsCPousRK"
      },
      "outputs": [],
      "source": [
        "#-------------------------------------------------------------------------------------\n",
        "filename = \"./dataset/bitcoin/bitcoin_60min_dataset.csv\"\n",
        "cryto_name = \"bitcoin\"\n",
        "cryto_data = \"bitcoin_60min\"\n",
        "from_date = \"2022-04-01\"\n",
        "to_date = \"2022-09-30\"\n",
        "output=\"./output/\"+cryto_name+\"/\"+cryto_data+\"/\"\n",
        "#os.mkdir(output)\n",
        "#-------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Execution of Prediction Model"
      ],
      "metadata": {
        "id": "jqAQVM6w_ctP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jto0_nTWuxdq",
        "outputId": "632d98ea-25fa-40e0-f138-b7a1c6ac6e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25953 entries, 0 to 25952\n",
            "Data columns (total 8 columns):\n",
            " #   Column      Non-Null Count  Dtype  \n",
            "---  ------      --------------  -----  \n",
            " 0   Date        25953 non-null  object \n",
            " 1   Open        25953 non-null  float64\n",
            " 2   High        25953 non-null  float64\n",
            " 3   Low         25953 non-null  float64\n",
            " 4   Close       25953 non-null  float64\n",
            " 5   SMAVG_50d   25904 non-null  float64\n",
            " 6   SMAVG_100d  25854 non-null  float64\n",
            " 7   SMAVG_200d  25754 non-null  float64\n",
            "dtypes: float64(7), object(1)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "Missing Values:\n",
            "Date            0\n",
            "Open            0\n",
            "High            0\n",
            "Low             0\n",
            "Close           0\n",
            "SMAVG_50d      49\n",
            "SMAVG_100d     99\n",
            "SMAVG_200d    199\n",
            "dtype: int64\n",
            "Duplicate Values:\n",
            "Empty DataFrame\n",
            "Columns: [Date, Open, High, Low, Close, SMAVG_50d, SMAVG_100d, SMAVG_200d]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# Set the sequence length - this is the timeframe used to make a single prediction\n",
        "sequence_length = 50\n",
        "data=load_time_series_data_and_preprocessing(filename,from_date,to_date)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "WYkTwndevEaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dda6ef8-ae23-4c5e-f4a3-8a7117f0df36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "np_data.shape: (4369, 6)\n"
          ]
        }
      ],
      "source": [
        "data_prices, data_prices_ext, np_Close_scaled,np_data_scaled,scaler_pred = feature_selection_scaling(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "yQS6pdZQvHKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db49b52b-155d-485f-9157-7da16b120475"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape: (3446, 50, 6) y_train.shape: (3446,)\n",
            "x_test.shape: (873, 50, 6) y_test.shape: (873,)\n",
            "0.9625339111150382\n",
            "0.9625339111150382\n"
          ]
        }
      ],
      "source": [
        "x_train, y_train, x_test, y_test,train_data_len = transform_multivariate_data(data,np_data_scaled,sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFWIxDmfvJMs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befd0f64-728c-4f6d-f91b-b0764fea3878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM Regression Summary :\n",
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_68 (LSTM)              (None, 50, 16)            1472      \n",
            "                                                                 \n",
            " dropout_68 (Dropout)        (None, 50, 16)            0         \n",
            "                                                                 \n",
            " lstm_69 (LSTM)              (None, 16)                2112      \n",
            "                                                                 \n",
            " dropout_69 (Dropout)        (None, 16)                0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,601\n",
            "Trainable params: 3,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/40\n",
            "38/38 [==============================] - 8s 82ms/step - loss: 0.0772 - root_mean_squared_error: 0.2779 - val_loss: 0.0011 - val_root_mean_squared_error: 0.0328\n",
            "Epoch 2/40\n",
            "38/38 [==============================] - 2s 49ms/step - loss: 0.0149 - root_mean_squared_error: 0.1220 - val_loss: 6.2671e-04 - val_root_mean_squared_error: 0.0250\n",
            "Epoch 3/40\n",
            "37/38 [============================>.] - ETA: 0s - loss: 0.0093 - root_mean_squared_error: 0.0967"
          ]
        }
      ],
      "source": [
        "history, results = train_multivariate_prediction_model(x_train, y_train,output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9PGo4tIwJeH"
      },
      "outputs": [],
      "source": [
        "config = [[16, 32], [32, 64, 128], [0.1,0.2]]  \n",
        "\n",
        "# list of lists --> [[first_additional_layer], [second_additional_layer], [third_additional_layer], [n_neurons], [n_batch_size], [dropout]]\n",
        "hist = tune_model_hyperparam(config, x_train, y_train, x_test, y_test,output)  # change x_train shape\n",
        "hist = pd.DataFrame(hist)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mknw4VA3HkGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist"
      ],
      "metadata": {
        "id": "zhxJEQIMHlB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = hist.sort_values(by=[4], ascending=True)\n",
        "hist.head(12)"
      ],
      "metadata": {
        "id": "nRxetOhLIH7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(f'Best Combination: \\n n_neurons = {hist.iloc[0, 0]}\\n n_batch_size = {hist.iloc[0, 1]}\\n dropout = {hist.iloc[0, 2]}')\n",
        "print('**************************')\n",
        "print(f'Results Before Tunning:\\n Test Set RMSE: {np.round(results, 4)[1]}\\n')\n",
        "print(f'Results After Tunning:\\n Test Set RMSE: {np.round(hist.iloc[0, -1], 4)[1]}\\n')\n",
        "print(f'{np.round((results[1] - hist.iloc[0, -1][1])*100/np.round(results, 4)[1])}% Improvement') "
      ],
      "metadata": {
        "id": "qRt5IXOrHdtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3HZcd_Sw0kZ"
      },
      "outputs": [],
      "source": [
        "n_neurons, n_batch_size, dropout = list(hist.iloc[0, :-2])\n",
        "print(list(hist.iloc[0, :-2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mo4K4s3Gw27D"
      },
      "outputs": [],
      "source": [
        "regressor,history = train_model_with_optimized_hyperparam(n_neurons, n_batch_size, dropout,output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myfsIjhNZa9q"
      },
      "outputs": [],
      "source": [
        "evalute_model_performance(history,output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoP-M5mgw6n4"
      },
      "outputs": [],
      "source": [
        "predict_future_price(x_test,y_test,train_data_len,from_date,cryto_name,output,regressor)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "i2gGxvbjAAL6",
        "mJ9cwxda_6ky",
        "AnBYBm94_zMM",
        "EnZNe1k1_Jku",
        "GGU_3sHIAPSy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}